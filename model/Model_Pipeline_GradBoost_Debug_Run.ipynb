{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import ALS Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import als_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run dataprep code to prepare data; import geocoder and manually fix one zip code missing due to query limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carkeek Warmer Carkeek Park, Seattle, WA <[OVER_QUERY_LIMIT] Google - Geocode [empty]> None\n"
     ]
    }
   ],
   "source": [
    "run dataprep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually add the one missing zipcode\n",
    "g = geocoder.google(\"Carkeek Park, Seattle, WA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<[OK] Google - Geocode [Carkeek Park, 950 NW Carkeek Park Rd, Seattle, WA 98177, USA]>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df['Venue_Zip2'] = cleaned_df[['Venue_Zip', 'Event_Name']].apply(lambda row: 98177 if row[1]=='Carkeek Warmer'\n",
    "                                                                       else row[0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned_df for future use without having to rerun dataprep file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle the cleaned_df for future use without having to re-run dataprep.py \n",
    "#(this will override the version automatically saved) by dataprep.py with the one with the corrected zipcode\n",
    "with open ('cleaned_df.pkl', 'w') as f:\n",
    "    pickle.dump(cleaned_df, f)\n",
    "    \n",
    "#to load this file in the future, run command:\n",
    "#with open('cleaned_df.pkl', 'rb') as f:\n",
    "    #dataframe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Class Instance for ALS Model, Prep and Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_model = als_model.implicit_als(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PersonID: bigint, EventID: bigint, Participated: bigint, Event_Date: bigint, SeriesID: double, EventTypeID: bigint, Total_Fee_Avg: bigint, Miles2_Avg: bigint, Venue_Zip: bigint]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_model.prep_spark_full_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------------+-------------------+--------+-----------+-------------+----------+---------+\n",
      "|PersonID|EventID|Participated|         Event_Date|SeriesID|EventTypeID|Total_Fee_Avg|Miles2_Avg|Venue_Zip|\n",
      "+--------+-------+------------+-------------------+--------+-----------+-------------+----------+---------+\n",
      "|       1|     11|           1|1423958400000000000|     0.0|          1|           46|        12|    98239|\n",
      "|       2|     11|           0|1423958400000000000|     0.0|          1|           46|        12|    98239|\n",
      "|       3|     11|           0|1423958400000000000|     0.0|          1|           46|        12|    98239|\n",
      "|       4|     11|           0|1423958400000000000|     0.0|          1|           46|        12|    98239|\n",
      "|       5|     11|           0|1423958400000000000|     0.0|          1|           46|        12|    98239|\n",
      "+--------+-------+------------+-------------------+--------+-----------+-------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "als_model.spark_full_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 592450.0\n",
      "Validation Size: 148113.0\n",
      "Test Size: 185141.0\n"
     ]
    }
   ],
   "source": [
    "als_model.train_val_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participants in train: 10764\n",
      "participants in validate: 10764\n",
      "participants in test: 10764\n",
      "\n",
      "\n",
      "participants in both train & validate: 10764\n",
      "participants in both train & test: 10764\n",
      "\n",
      "\n",
      "EventID in train: 56\n",
      "EventID in validate: 14\n",
      "EventID in test: 18\n",
      "\n",
      "\n",
      "EventID in both train & validate: 1\n",
      "EventID in both train & test: 0\n"
     ]
    }
   ],
   "source": [
    "als_model.print_train_val_test_info(\"EventID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_model.create_participate_matrices(\"EventID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on Training Data:\n",
      "Predictions includes 592450 valid values and 0 nan values\n",
      "\n",
      "\n",
      "Mean prediction is 0.0303024963926\n",
      "Error of Type r2 = 0.317162022448\n",
      "Predictions on Validation Data:\n",
      "Predictions includes 10334 valid values and 137779 nan values\n",
      "\n",
      "\n",
      "Mean prediction is 0.00201103141265\n",
      "Error of Type r2 = -0.0142867802634\n"
     ]
    }
   ],
   "source": [
    "als_model.run_ALS_TVS(event_param=\"EventID\", scoring=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "parammap = als_model.tvs_model.getEstimatorParamMaps()\n",
    "avgmetrics = als_model.tvs_model.validationMetrics\n",
    "\n",
    "zipped_tvs_model_info = zip(parammap, avgmetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.14203634369376705, -0.14203634369376705)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(avgmetrics), max(avgmetrics)\n",
    "#All combinations of parameters give the same validation number...how can this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save spark_full_df, bestmodel from ALS TVS run, and predictions on training data from that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_pd = als_model.spark_full_df.toPandas()\n",
    "als_fitted_model_tvsr2 = als_model.tvs_bestmodel\n",
    "als_trainpreds_tvsr2_pd = als_model.tvs_trainpreds.toPandas()\n",
    "\n",
    "with open (\"24OCT17_pipelinerun/full_df_pd.pkl\", 'w') as f:\n",
    "    pickle.dump(full_df_pd, f)\n",
    "\n",
    "als_fitted_model_tvsr2.save(\"24OCT17_pipelinerun/als_fitted_model_tvsr2\")\n",
    "\n",
    "with open (\"24OCT17_pipelinerun/als_trainpreds_tvsr2.pkl\", 'w') as f:\n",
    "    pickle.dump(als_trainpreds_tvsr2_pd, f)\n",
    "\n",
    "#Note: to load the model saved above in future, run command:\n",
    "#sameModel = GradientBoostedTreesModel.load(\"tvs_bestmodel_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "als_train_df_pd = als_model.train.toPandas()\n",
    "als_val_df_pd = als_model.validate.toPandas()\n",
    "als_test_df_pd = als_model.test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open (\"24OCT17_pipelinerun/als_train_df_pd.pkl\", 'w') as f:\n",
    "    pickle.dump(als_train_df_pd, f)\n",
    "    \n",
    "with open (\"24OCT17_pipelinerun/als_val_df_pd.pkl\", 'w') as f:\n",
    "    pickle.dump(als_val_df_pd, f)\n",
    "    \n",
    "with open (\"24OCT17_pipelinerun/als_test_df_pd.pkl\", 'w') as f:\n",
    "    pickle.dump(als_test_df_pd, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and run code to prepare training data for gradient boosted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prep_gbdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_dataprep = prep_gbdata.RegressionDataPrep(spark_data_df=als_model.train, user_df=cleaned_df, datasplit='train', \n",
    "                                             als_predictions=als_model.tvs_trainpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_dataprep.format_gb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event_Date</th>\n",
       "      <th>Total_Fee_Avg</th>\n",
       "      <th>Miles2_Avg</th>\n",
       "      <th>AgeAvg</th>\n",
       "      <th>y_label</th>\n",
       "      <th>SeriesID_1.0</th>\n",
       "      <th>SeriesID_2.0</th>\n",
       "      <th>SeriesID_3.0</th>\n",
       "      <th>SeriesID_4.0</th>\n",
       "      <th>EventTypeID_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Venue_Zip_98118</th>\n",
       "      <th>Venue_Zip_98177</th>\n",
       "      <th>Venue_Zip_98208</th>\n",
       "      <th>Venue_Zip_98239</th>\n",
       "      <th>Venue_Zip_98290</th>\n",
       "      <th>Venue_Zip_98332</th>\n",
       "      <th>Venue_Zip_98922</th>\n",
       "      <th>Venue_Zip_99032</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Gender_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1418428800000000000</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1418428800000000000</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1418428800000000000</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>0.030626</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1418428800000000000</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1418428800000000000</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Event_Date  Total_Fee_Avg  Miles2_Avg  AgeAvg   y_label  \\\n",
       "0  1418428800000000000             39           8      36  0.000247   \n",
       "1  1418428800000000000             39           8      41  0.000000   \n",
       "2  1418428800000000000             39           8      15  0.030626   \n",
       "3  1418428800000000000             39           8      28  0.000000   \n",
       "4  1418428800000000000             39           8      20  0.000000   \n",
       "\n",
       "   SeriesID_1.0  SeriesID_2.0  SeriesID_3.0  SeriesID_4.0  EventTypeID_2  \\\n",
       "0             0             0             0             1              1   \n",
       "1             0             0             0             1              1   \n",
       "2             0             0             0             1              1   \n",
       "3             0             0             0             1              1   \n",
       "4             0             0             0             1              1   \n",
       "\n",
       "       ...       Venue_Zip_98118  Venue_Zip_98177  Venue_Zip_98208  \\\n",
       "0      ...                     0                0                0   \n",
       "1      ...                     0                0                0   \n",
       "2      ...                     0                0                0   \n",
       "3      ...                     0                0                0   \n",
       "4      ...                     0                0                0   \n",
       "\n",
       "   Venue_Zip_98239  Venue_Zip_98290  Venue_Zip_98332  Venue_Zip_98922  \\\n",
       "0                0                0                0                0   \n",
       "1                0                0                0                0   \n",
       "2                0                0                0                0   \n",
       "3                0                0                0                0   \n",
       "4                0                0                0                0   \n",
       "\n",
       "   Venue_Zip_99032  Gender_Male  Gender_Other  \n",
       "0                0            1             0  \n",
       "1                0            0             0  \n",
       "2                0            0             0  \n",
       "3                0            1             0  \n",
       "4                0            0             0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_dataprep.train_gb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"24OCT17_pipelinerun/gb_train_data.pkl\", 'w') as f:\n",
    "    pickle.dump(gb_dataprep.train_gb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Gradient Boosted Regressor out of the box using prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open (\"24OCT17_pipelinerun/gb_train_data.pkl\", 'rb') as f:\n",
    "    gb_train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = gb_train_data['y_label']\n",
    "X = gb_train_data.drop('y_label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024883492158110698"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model.score(X, y)\n",
    "#why is this so bad when run on the training data???  Note that this score method has 1.0 as the best possible score.\n",
    "#could this be due to the imbalance in my data (vast majority of observations are close to zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Validation Data for Gradient Boosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep validation data (note: need to go back and refit ALS model with train+validate once finalize it's tuning, then \n",
    "#will run the gb test on the final test data; for now using the validation data though...):\n",
    "gb_valdataprep = prep_gbdata.RegressionDataPrep(spark_data_df=als_model.validate, user_df=cleaned_df, \n",
    "                                                datasplit='validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_valdataprep.format_gb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open (\"24OCT17_pipelinerun/gb_val_data.pkl\", 'w') as f:\n",
    "    pickle.dump(gb_valdataprep.train_gb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Predict on Gradient Boosted Model Using Validation Data to Test Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val = gb_valdataprep.train_gb['y_label']\n",
    "X_val = gb_valdataprep.train_gb.drop('y_label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_msg_in_val = set(X.columns) - set(X_val.columns)\n",
    "cols_msg_in_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_msg_in_val:\n",
    "    X_val[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Venue_Zip_98077'}"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(X_val.columns) - set(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val.drop('Venue_Zip_98077', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_predictions = gb_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.032974955711363441"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
